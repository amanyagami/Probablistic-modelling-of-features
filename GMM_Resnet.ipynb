{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.resnet import ResNet34\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import sklearn.covariance\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from joblib import load ,dump\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = ResNet34(10).to(device)\n",
    "checkpoint = torch.load(\"./pre_trained/resnet34_cifar10.pth\")\n",
    "new_state_dict = OrderedDict()\n",
    "for k,v in checkpoint.items():\n",
    "    new_key = k.replace('module.','')\n",
    "    new_state_dict[new_key] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ID data -> Cifar 10\n",
    "Profiling(mean and covariance is done on Cifar 10)\n",
    "\"\"\"\n",
    "data_path = \"./data\"\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalize using CIFAR-10 mean and std\n",
    "])\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',  # Path to the dataset folder\n",
    "    train=False,    # Indicate this is the test dataset\n",
    "    download=True,  # Download the dataset if not available\n",
    "    transform=transform_test  # Apply the transformations\n",
    ")\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',  # Path to the dataset folder\n",
    "    train=True,    # Indicate this is the test dataset\n",
    "    download=True,  # Download the dataset if not available\n",
    "    transform=transform_test  # Apply the transformations\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64,   # Set the batch size (you can adjust this)\n",
    "    shuffle=False,   # Do not shuffle for testing\n",
    "    num_workers=2    # Number of subprocesses to use for data loading\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,   # Set the batch size (you can adjust this)\n",
    "    shuffle=False,   # Do not shuffle for testing\n",
    "    num_workers=2    # Number of subprocesses to use for data loading\n",
    ")\n",
    "\n",
    "# Create test datasets\n",
    "svhn_test_dataset = torchvision.datasets.SVHN(root=data_path, split='test', download=True, transform=transform_test)\n",
    "isun_test_dataset = torchvision.datasets.ImageFolder(root=f\"{data_path}/isun\", transform=transform_test)\n",
    "lsun_test_dataset = torchvision.datasets.ImageFolder(root=f\"{data_path}/lsun_resize\", transform=transform_test)\n",
    "\n",
    "# Create test data loaders\n",
    "batch_size = 64  # Set your batch size here\n",
    "svhn_test_loader = DataLoader(svhn_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "isun_test_loader = DataLoader(isun_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "lsun_test_loader = DataLoader(lsun_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "autoAttack_data = torch.load('./adv_samples/densenet3_cifar10_autoattack.pt')\n",
    "data_list, label_list = zip(*autoAttack_data)\n",
    "inputs_tensor = torch.stack([torch.from_numpy(data).float() for data in data_list])\n",
    "labels_tensor = torch.tensor(label_list, dtype=torch.long)\n",
    "\n",
    "AA_dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "Autoattack_test_loader = DataLoader(AA_dataset, batch_size=64, shuffle=False)\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "fgsm_data = torch.load('./adv_samples/densenet3_cifar10_fgsm.pt')\n",
    "data_list, label_list = zip(*fgsm_data)\n",
    "inputs_tensor = torch.stack([torch.from_numpy(data).float() for data in data_list])\n",
    "labels_tensor = torch.tensor(label_list, dtype=torch.long)\n",
    "fsgm_dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "fgsm_test_loader = DataLoader(fsgm_dataset, batch_size=64, shuffle=False)\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "pgd_data = torch.load('./adv_samples/densenet3_cifar10_pgd.pt')\n",
    "data_list, label_list = zip(*pgd_data)\n",
    "inputs_tensor = torch.stack([torch.from_numpy(data).float() for data in data_list])\n",
    "labels_tensor = torch.tensor(label_list, dtype=torch.long)\n",
    "pgd_dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "pgd_test_loader = DataLoader(pgd_dataset, batch_size=64, shuffle=False)\n",
    "# \"\"\"\n",
    "# extract Features -> get features of a specified dataset , using a model, and at a specific index\n",
    "\n",
    "# \"\"\"\n",
    "# def extract_features(loader, model):\n",
    "#     layer_index=4\n",
    "#     features0 = []\n",
    "#     features1 = []\n",
    "#     features2 = []\n",
    "#     features3 = []\n",
    "#     features4 = []\n",
    "#     labels = []\n",
    "#     model.eval()\n",
    "#     count = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in loader:\n",
    "#             count += 1\n",
    "#             inputs = inputs.cuda()\n",
    "#             layer_features4,layer_features3,layer_features2,layer_features1,layer_features0 = model.intermediate_forward(inputs, layer_index=layer_index)\n",
    "            \n",
    "#             layer_features0_flat = layer_features1.view(layer_features0.size(0), -1)\n",
    "#             features0.append(layer_features0_flat.cpu())\n",
    "\n",
    "#             layer_features1_flat = layer_features1.view(layer_features1.size(0), -1)\n",
    "#             features1.append(layer_features1_flat.cpu())\n",
    "\n",
    "#             layer_features2_flat = layer_features2.view(layer_features2.size(0), -1)\n",
    "#             features2.append(layer_features2_flat.cpu())\n",
    "            \n",
    "#             layer_features3_flat = layer_features3.view(layer_features3.size(0), -1)\n",
    "#             features3.append(layer_features3_flat.cpu())\n",
    "\n",
    "#             layer_features4_flat = layer_features1.view(layer_features4.size(0), -1)\n",
    "#             features4.append(layer_features4_flat.cpu())\n",
    "\n",
    "#             labels.append(targets.cpu())\n",
    "#                 # break\n",
    "#         features0 = torch.cat(features0, dim=0).numpy()\n",
    "#         features1 = torch.cat(features1, dim=0).numpy()\n",
    "#         features2 = torch.cat(features2, dim=0).numpy()\n",
    "#         features3 = torch.cat(features3, dim=0).numpy()\n",
    "#         features4 = torch.cat(features4, dim=0).numpy()\n",
    "#         labels = torch.cat(labels, dim=0).numpy() if len(labels) > 0 else None\n",
    "#     return features0 ,features1,features2, features3,features4,labels\n",
    "\n",
    "\n",
    "# ood_features_dict = {}\n",
    "\n",
    "test_loaders = {\n",
    "    \"ID\": test_loader,\n",
    "    \"ID_Train\": train_loader,\n",
    "    \"PGD\": pgd_test_loader,\n",
    "    \"FGSM\": fgsm_test_loader,\n",
    "    \"LSUN\": lsun_test_loader,\n",
    "    \"SVHN\": svhn_test_loader,\n",
    "    \"iSUN\":isun_test_loader,\n",
    "    \"AutoAttack\" : Autoattack_test_loader,\n",
    "}\n",
    "# # Initialize the dictionary to store features and labels\n",
    "\n",
    "# # Loop over each dataset\n",
    "# for dataset_name, loader in test_loaders.items():\n",
    "#     print(f\"Processing dataset: {dataset_name}\")\n",
    "#     features_dict = {}\n",
    "#     features_dict[dataset_name] = {}\n",
    "    \n",
    "   \n",
    "#     features0,features1,features2,features3,features4, labels = extract_features(loader, model)\n",
    "    \n",
    "#     # Store the extracted features and labels in the dictionary\n",
    "#     features_dict[dataset_name][0] = {\n",
    "#         \"features\": features0,\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "#     features_dict[dataset_name][1] = {\n",
    "#         \"features\": features1,\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "#     features_dict[dataset_name][2] = {\n",
    "#         \"features\": features2,\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "#     features_dict[dataset_name][3] = {\n",
    "#         \"features\": features3,\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "#     features_dict[dataset_name][4] = {\n",
    "#         \"features\": features4,\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "    \n",
    "#     # print(\"Feature extraction completed.\")\n",
    "\n",
    "#     dump(features_dict,f\"extracted_features_{dataset_name}.pkl\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_features_dict = {}\n",
    "\n",
    "#     loaded_features_dict[dataset_name] = load(f\"extracted_features_{dataset_name}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load ,dump\n",
    "# print(\"Feature extraction completed.\")\n",
    "\n",
    "# dump(features_dict0,\"extracted_features_all.pkl\")\n",
    "# dump(features_dict0,\"extracted_features_all.pkl\")\n",
    "# dump(features_dict0,\"extracted_features_all.pkl\")\n",
    "\n",
    "# import cudf\n",
    "# loaded_features_dict = load(\"extracted_features_100.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load ,dump\n",
    "loaded_features_dict = load(\"extracted_features_ID_Train.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "a =loaded_features_dict['ID_Train'].keys()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/ASUAD/asing651/.conda/envs/rapids-24.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ----------->\n",
      "fitting the umap model \n",
      "from len =  65536\n",
      " values reduced to  200\n",
      "1 ----------->\n",
      "fitting the umap model \n",
      "from len =  65536\n",
      " values reduced to  200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m feature_values \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(feature_values)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m values reduced to \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(feature_values[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 22\u001b[0m \u001b[43mgmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m gmm_final[layer_index]\u001b[38;5;241m=\u001b[39m gmm\n",
      "File \u001b[0;32m~/.conda/envs/rapids-24.10/lib/python3.10/site-packages/sklearn/mixture/_base.py:181\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/rapids-24.10/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rapids-24.10/lib/python3.10/site-packages/sklearn/mixture/_base.py:248\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    245\u001b[0m prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[1;32m    247\u001b[0m log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e_step(X)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_resp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(log_resp, log_prob_norm)\n\u001b[1;32m    251\u001b[0m change \u001b[38;5;241m=\u001b[39m lower_bound \u001b[38;5;241m-\u001b[39m prev_lower_bound\n",
      "File \u001b[0;32m~/.conda/envs/rapids-24.10/lib/python3.10/site-packages/sklearn/mixture/_gaussian_mixture.py:812\u001b[0m, in \u001b[0;36mGaussianMixture._m_step\u001b[0;34m(self, X, log_resp)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, log_resp):\n\u001b[1;32m    802\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"M step.\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \n\u001b[1;32m    804\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03m        the point of each sample in X.\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_ \u001b[38;5;241m=\u001b[39m \u001b[43m_estimate_gaussian_parameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_resp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreg_covar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance_type\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_ \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecisions_cholesky_ \u001b[38;5;241m=\u001b[39m _compute_precision_cholesky(\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_type\n\u001b[1;32m    818\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/rapids-24.10/lib/python3.10/site-packages/sklearn/mixture/_gaussian_mixture.py:290\u001b[0m, in \u001b[0;36m_estimate_gaussian_parameters\u001b[0;34m(X, resp, reg_covar, covariance_type)\u001b[0m\n\u001b[1;32m    288\u001b[0m nk \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(resp\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m    289\u001b[0m means \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(resp\u001b[38;5;241m.\u001b[39mT, X) \u001b[38;5;241m/\u001b[39m nk[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m--> 290\u001b[0m covariances \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_estimate_gaussian_covariances_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtied\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_estimate_gaussian_covariances_tied\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_estimate_gaussian_covariances_diag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspherical\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_estimate_gaussian_covariances_spherical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcovariance_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_covar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nk, means, covariances\n",
      "File \u001b[0;32m~/.conda/envs/rapids-24.10/lib/python3.10/site-packages/sklearn/mixture/_gaussian_mixture.py:177\u001b[0m, in \u001b[0;36m_estimate_gaussian_covariances_full\u001b[0;34m(resp, X, nk, means, reg_covar)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_components):\n\u001b[1;32m    176\u001b[0m     diff \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m-\u001b[39m means[k]\n\u001b[0;32m--> 177\u001b[0m     covariances[k] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nk[k]\n\u001b[1;32m    178\u001b[0m     covariances[k]\u001b[38;5;241m.\u001b[39mflat[:: n_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reg_covar\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m covariances\n",
      "File \u001b[0;32m~/.conda/envs/rapids-24.10/lib/python3.10/site-packages/numpy/_core/multiarray.py:750\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03m    result_type(*arrays_and_dtypes)\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays_and_dtypes\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mdot)\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(a, b, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;124;03m    dot(a, b, out=None)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    838\u001b[0m \n\u001b[1;32m    839\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b, out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA,IncrementalPCA\n",
    "import umap \n",
    "\n",
    "gmm_final = {} \n",
    "umap_final = {}\n",
    "new_dim = {}\n",
    "for layer_index in loaded_features_dict['ID_Train'].keys():\n",
    "    # umap_model = umap.UMAP(n_components=700, n_neighbors=700,min_dist=0.5)\n",
    "    gmm_final[layer_index]= {}\n",
    "    gmm = GaussianMixture(n_components=10)\n",
    "    print(layer_index, \"----------->\")\n",
    "    print(\"fitting the umap model \")\n",
    "    feature_values = loaded_features_dict['ID_Train'][layer_index][\"features\"]\n",
    "    print(\"from len = \",len(feature_values[0]))\n",
    "    pca = PCA(n_components=200)\n",
    "    pca = pca.fit(feature_values)\n",
    "    feature_values = pca.transform(feature_values)\n",
    "    print(\" values reduced to \", len(feature_values[0]))\n",
    "    gmm.fit(feature_values)\n",
    "    gmm_final[layer_index]= gmm\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMM scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([1,2,3,4,5,6,7,8,9])\n",
    "a =a.reshape(3,3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmm_final)\n",
    "test_loaders = [\"ID\" ,    \"PGD\" ,    \"FGSM\" ,    \"LSUN\",    \"SVHN\" ,    \"iSUN\" ,    \"AutoAttack\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "gmm_scores = {}\n",
    "for data in test_loaders:\n",
    "   gmm_scores[data] = {}\n",
    "   loaded_features_dict = load(f\"extracted_features_{data}.pkl\")\n",
    "   for layer_index in loaded_features_dict[data].keys():\n",
    "      gmm =  gmm_final[layer_index]\n",
    "   #    print(loaded_features_dict[data][layer_index])\n",
    "      \n",
    "      feature_values = loaded_features_dict[data][layer_index]['features']\n",
    "      print(data,\" -... \")\n",
    "      pca = PCA(n_components=200)\n",
    "      feature_values =  pca.fit_transform(feature_values)\n",
    "      print(\"converted to lower dimenion\")\n",
    "      print(feature_values[0].shape)\n",
    "      gmm_scores[data][layer_index] = gmm.score_samples(feature_values)\n",
    "      print(f\"score generated for {layer_index}, for data = {data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(gmm_scores['SVHN'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rows = []\n",
    "for data_type, layers in gmm_scores.items():\n",
    "    for i in range(len(next(iter(layers.values())))):  # Assumes all layers have the same length\n",
    "        row = [data_type]\n",
    "        for layer_index in range(5):  # Assuming 5 layer indices\n",
    "            row.append(layers[layer_index][i])\n",
    "        rows.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "headers = [\"data_type\", \"layer_index1\", \"layer_index2\", \"layer_index3\", \"layer_index4\", \"layer_index5\"]\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "df.to_csv(\"gmm_scores11thdec.csv\", index=False)\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "id_df = df[df['data_type'] == 'ID'] \n",
    "thresholds = {}\n",
    "layer_columns = [\"layer_index1\", \"layer_index2\", \"layer_index3\", \"layer_index4\", \"layer_index5\"]\n",
    "for col in layer_columns:\n",
    "    threshold = id_df[col].quantile(0.05)  # 5th percentile\n",
    "    thresholds[col] = threshold\n",
    "\n",
    "print(\"Thresholds for 95% TPR:\")\n",
    "for layer, thresh in thresholds.items():\n",
    "    print(f\"{layer}: {thresh}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "\n",
    "def compute_metrics_for_data_type(df, thresholds, test_data_type):\n",
    "    \"\"\"\n",
    "    Computes Accuracy, AUROC, AUPR for each layer and adds a column counting\n",
    "    the number of layers that correctly classified each sample.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataset containing 'data_type' and layer scores.\n",
    "    - thresholds (dict): Dictionary with layer names as keys and threshold values as values.\n",
    "    - test_data_type (str): The specific OOD data type to evaluate against 'ID'.\n",
    "\n",
    "    Returns:\n",
    "    - metrics_result (dict): Dictionary containing metrics for each layer.\n",
    "    - subset (pd.DataFrame): The subset DataFrame with the new 'correct_layer_count' column.\n",
    "    \"\"\"\n",
    "    # Filter for ID and the test_data_type\n",
    "    subset = df[(df['data_type'] == 'ID') | (df['data_type'] == test_data_type)].copy()\n",
    "    \n",
    "    # Create binary ground truth: 1 for ID, 0 for non-ID\n",
    "    y_true = (subset['data_type'] == 'ID').astype(int)\n",
    "    \n",
    "    metrics_result = {}\n",
    "    \n",
    "    # Initialize a DataFrame to store binary predictions for each layer\n",
    "    predictions = pd.DataFrame(index=subset.index)\n",
    "    \n",
    "    for layer_col, threshold in thresholds.items():\n",
    "        # Extract scores for this layer\n",
    "        y_scores = subset[layer_col].values\n",
    "        \n",
    "        # Binary predictions based on threshold\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        \n",
    "        # Store predictions in the DataFrame\n",
    "        predictions[layer_col] = y_pred\n",
    "        \n",
    "        \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        # Compute AUROC\n",
    "        try:\n",
    "            auroc = roc_auc_score(y_true, y_scores)\n",
    "        except ValueError:\n",
    "            auroc = float('nan')\n",
    "        \n",
    "        # Compute AUPR\n",
    "        try:\n",
    "            aupr = average_precision_score(y_true, y_scores)\n",
    "        except ValueError:\n",
    "            aupr = float('nan')\n",
    "        \n",
    "        # Store metrics for this layer\n",
    "        metrics_result[layer_col] = {\n",
    "            'accuracy': acc,\n",
    "            'auroc': auroc,\n",
    "            'aupr': aupr\n",
    "        }\n",
    "    correct_predictions = predictions.eq(y_true, axis=0)  # DataFrame of booleans\n",
    "    subset['correct_layer_count'] = correct_predictions.sum(axis=1)\n",
    "    \n",
    "    return metrics_result, subset\n",
    "\n",
    "# Example usage:\n",
    "# Assume we have a DataFrame 'df' and a dictionary 'thresholds' defined elsewhere\n",
    "# Replace 'OOD' with whatever non-ID data_type you want to evaluate\n",
    "test_data_types = ['SVHN','iSUN','LSUN','AutoAttack','PGD','FGSM','ID']\n",
    "\n",
    "# Initialize a dictionary to store all metrics for each test_data_type\n",
    "all_metrics = {}\n",
    "\n",
    "for d_type in test_data_types:\n",
    "    metrics_for_ood, subset_with_counts = compute_metrics_for_data_type(df, thresholds, d_type)\n",
    "    all_metrics[d_type] = metrics_for_ood\n",
    "    print(f\"\\nData Type: {d_type}\")\n",
    "    for layer, vals in metrics_for_ood.items():\n",
    "        print(f\"{layer} vs ID:\")\n",
    "        print(f\"  Accuracy: {vals['accuracy']:.4f}  ,  AUROC: {vals['auroc']:.4f}  ,  AUPR: {vals['aupr']:.4f}\")\n",
    "    \n",
    "\n",
    "\n",
    "df.loc[subset_with_counts.index, 'correct_layer_count_' + d_type] = subset_with_counts['correct_layer_count']\n",
    "\n",
    "# After the loop, 'df' will have new columns like 'correct_layer_count_SVHN', etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Assuming 'df' is your DataFrame\n",
    "df_long = df.melt(id_vars='data_type', \n",
    "                  var_name='layer', \n",
    "                  value_name='score')\n",
    "\n",
    "# List of layer columns\n",
    "layers = [\"layer_index1\", \"layer_index2\", \"layer_index3\", \"layer_index4\", \"layer_index5\"]\n",
    "\n",
    "for layer in layers:\n",
    "   \n",
    "    filtered_df = df\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(data=filtered_df, x='data_type', y=layer, hue='data_type', palette='deep', alpha=0.7)\n",
    "    plt.title(f'Scatter Plot for {layer}')\n",
    "    plt.xlabel('Data Type')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(title='Data Type')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the new dictionary for t-SNE representations\n",
    "# tsne_features_dict = {}\n",
    "\n",
    "# # Set t-SNE parameters (you can adjust these as needed)\n",
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "\n",
    "# # Iterate through test loaders\n",
    "# for test_loader_name, layer_data in loaded_features_dict.items():\n",
    "#     tsne_features_dict[test_loader_name] = {}\n",
    "    \n",
    "#     for layer_index, layer_info in layer_data.items():\n",
    "#         # Get features\n",
    "#         features = layer_info[\"features\"]\n",
    "        \n",
    "#         # Ensure features are in a compatible format (e.g., numpy array)\n",
    "#         features = np.array(features)\n",
    "        \n",
    "#         # Compute t-SNE\n",
    "#         tsne_representation = tsne.fit_transform(features)\n",
    "        \n",
    "#         # Save t-SNE results to the new dictionary\n",
    "#         tsne_features_dict[test_loader_name][layer_index] = {\n",
    "#             \"tsne_representation\": tsne_representation\n",
    "#         }\n",
    "\n",
    "# # Now tsne_features_dict contains t-SNE representations for each test_loader and layer_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # Assume `model` is your ResNet instance and `train_loader` is your DataLoader with 100 samples from 10 classes.\n",
    "\n",
    "# # Initialize storage for features and labels\n",
    "# features_0 = []\n",
    "# labels = []\n",
    "# layer_index = 0\n",
    "# # Extract features from layer 1\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs = inputs.cuda()\n",
    "#         layer1_features = model.intermediate_forward(inputs, layer_index=layer_index)\n",
    "#         # Flatten the features if they are multi-dimensional (e.g., [batch, channels, height, width])\n",
    "#         layer1_features_flat = layer1_features.view(layer1_features.size(0), -1)\n",
    "#         features_0.append(layer1_features_flat.cpu())\n",
    "#         labels.append(targets.cpu())\n",
    "#         if len(labels) > 500:\n",
    "#             break\n",
    "\n",
    "# # Concatenate all features and labels\n",
    "# features_0 = torch.cat(features_0, dim=0).numpy()\n",
    "# labels = torch.cat(labels, dim=0).numpy()\n",
    "\n",
    "# # PCA and t-SNE feature reduction\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_features = pca.fit_transform(features_0)\n",
    "\n",
    "# tsne = TSNE(n_components=2, perplexity=30, random_state=0)\n",
    "# tsne_features = tsne.fit_transform(features_0)\n",
    "\n",
    "# # Plotting all features together\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# # PCA Plot\n",
    "# axes[0].scatter(pca_features[:, 0], pca_features[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# axes[0].set_title(\"PCA Feature Visualization\")\n",
    "# axes[0].set_xlabel(\"Dimension 1\")\n",
    "# axes[0].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# # t-SNE Plot\n",
    "# axes[1].scatter(tsne_features[:, 0], tsne_features[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# axes[1].set_title(\"t-SNE Feature Visualization\")\n",
    "# axes[1].set_xlabel(\"Dimension 1\")\n",
    "# axes[1].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting each class separately\n",
    "# unique_labels = sorted(set(labels))\n",
    "# num_classes = len(unique_labels)\n",
    "\n",
    "# # Create separate plots for each class\n",
    "# fig, axes = plt.subplots(num_classes, 2, figsize=(15, 5 * num_classes))\n",
    "# if num_classes == 1:\n",
    "#     axes = [axes]  # Ensure axes is iterable for a single class\n",
    "\n",
    "# for i, label in enumerate(unique_labels):\n",
    "#     class_mask = labels == label\n",
    "#     class_pca_features = pca_features[class_mask]\n",
    "#     class_tsne_features = tsne_features[class_mask]\n",
    "#     # PCA Plot for this class\n",
    "#     axes[i][0].scatter(class_pca_features[:, 0], class_pca_features[:, 1], cmap='tab10', alpha=0.7)\n",
    "#     axes[i][0].set_title(f\"PCA Feature Visualization - Class {label} , layer = {layer_index}\")\n",
    "#     axes[i][0].set_xlabel(\"Dimension 1\")\n",
    "#     axes[i][0].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "#     # t-SNE Plot for this class\n",
    "#     axes[i][1].scatter(class_tsne_features[:, 0], class_tsne_features[:, 1], cmap='tab10', alpha=0.7)\n",
    "#     axes[i][1].set_title(f\"t-SNE Feature Visualization - Class {label}\")\n",
    "#     axes[i][1].set_xlabel(\"Dimension 1\")\n",
    "#     axes[i][1].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # Initialize storage for OOD features from all test loaders\n",
    "# ood_features_dict = {}\n",
    "\n",
    "# # Helper function to extract features\n",
    "# def extract_features(loader, model, layer_index=0):\n",
    "#     features = []\n",
    "#     labels = []\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in loader:\n",
    "#             inputs = inputs.cuda()\n",
    "#             layer_features = model.intermediate_forward(inputs, layer_index=layer_index)\n",
    "#             layer_features_flat = layer_features.view(layer_features.size(0), -1)\n",
    "#             features.append(layer_features_flat.cpu())\n",
    "#             labels.append(targets.cpu())\n",
    "#         features = torch.cat(features, dim=0).numpy()\n",
    "#         labels = torch.cat(labels, dim=0).numpy() if len(labels) > 0 else None\n",
    "#     return features, labels\n",
    "\n",
    "# # Extract features for original dataset (in-distribution features)\n",
    "# features_id, labels_id = extract_features(train_loader, model)\n",
    "\n",
    "# # Extract OOD features from each test loader\n",
    "# test_loaders = {\n",
    "#     \"PGD\": pgd_test_loader,\n",
    "#     \"FGSM\": fgsm_test_loader,\n",
    "#     \"LSUN\": lsun_test_loader,\n",
    "#     \"SVHN\": DataLoader(svhn_test_dataset, batch_size=64, shuffle=False),\n",
    "#     \"iSUN\": DataLoader(isun_test_dataset, batch_size=64, shuffle=False),\n",
    "# }\n",
    "\n",
    "# for name, loader in test_loaders.items():\n",
    "#     ood_features, _ = extract_features(loader, model)\n",
    "#     ood_features_dict[name] = ood_features\n",
    "\n",
    "# # Dimensionality Reduction with PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_features_id = pca.fit_transform(features_id)\n",
    "# pca_features_ood_dict = {name: pca.transform(ood_features) for name, ood_features in ood_features_dict.items()}\n",
    "\n",
    "# # Dimensionality Reduction with t-SNE\n",
    "# tsne = TSNE(n_components=2, perplexity=30, random_state=0)\n",
    "# tsne_features_id = tsne.fit_transform(features_id)\n",
    "# tsne_features_ood_dict = {name: tsne.fit_transform(ood_features) for name, ood_features in ood_features_dict.items()}\n",
    "\n",
    "# # Plotting PCA and t-SNE visualizations\n",
    "# unique_classes = sorted(set(labels_id))\n",
    "# num_ood = len(pca_features_ood_dict)\n",
    "\n",
    "# # Creating separate grids for PCA and t-SNE\n",
    "# fig_pca, axes_pca = plt.subplots(len(unique_classes), num_ood, figsize=(15, 5 * len(unique_classes)))\n",
    "# fig_tsne, axes_tsne = plt.subplots(len(unique_classes), num_ood, figsize=(15, 5 * len(unique_classes)))\n",
    "\n",
    "# if len(unique_classes) == 1:\n",
    "#     axes_pca = [axes_pca]  # Ensure axes is iterable for a single class\n",
    "#     axes_tsne = [axes_tsne]\n",
    "\n",
    "# for i, class_label in enumerate(unique_classes):\n",
    "#     class_mask = labels_id == class_label\n",
    "#     class_features_id_pca = pca_features_id[class_mask]\n",
    "#     class_features_id_tsne = tsne_features_id[class_mask]\n",
    "\n",
    "#     for j, (ood_name, pca_features_ood) in enumerate(pca_features_ood_dict.items()):\n",
    "#         # PCA Plot\n",
    "#         axes_pca[i][j].scatter(class_features_id_pca[:, 0], class_features_id_pca[:, 1], alpha=0.7, label=f\"Class {class_label}\")\n",
    "#         axes_pca[i][j].scatter(pca_features_ood[:, 0], pca_features_ood[:, 1], alpha=0.7, label=f\"{ood_name} (OOD)\", c='red')\n",
    "#         axes_pca[i][j].set_title(f\"PCA: Class {class_label} vs {ood_name} (OOD)\")\n",
    "#         axes_pca[i][j].set_xlabel(\"PCA Dimension 1\")\n",
    "#         axes_pca[i][j].set_ylabel(\"PCA Dimension 2\")\n",
    "#         axes_pca[i][j].legend()\n",
    "\n",
    "#         # t-SNE Plot\n",
    "#         tsne_features_ood = tsne_features_ood_dict[ood_name]\n",
    "#         axes_tsne[i][j].scatter(class_features_id_tsne[:, 0], class_features_id_tsne[:, 1], alpha=0.7, label=f\"Class {class_label}\")\n",
    "#         axes_tsne[i][j].scatter(tsne_features_ood[:, 0], tsne_features_ood[:, 1], alpha=0.7, label=f\"{ood_name} (OOD)\", c='red')\n",
    "#         axes_tsne[i][j].set_title(f\"t-SNE: Class {class_label} vs {ood_name} (OOD)\")\n",
    "#         axes_tsne[i][j].set_xlabel(\"t-SNE Dimension 1\")\n",
    "#         axes_tsne[i][j].set_ylabel(\"t-SNE Dimension 2\")\n",
    "#         axes_tsne[i][j].legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # Assume `model` is your ResNet instance and `data_loader` is your DataLoader with 100 samples from 10 classes.\n",
    "\n",
    "# # Initialize storage for features and labels\n",
    "# features_4 = []\n",
    "# labels = []\n",
    "# layer_index = 4\n",
    "# # Extract features from layer 1\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs = inputs.cuda()\n",
    "#         layer1_features = model.intermediate_forward(inputs, layer_index=layer_index)\n",
    "#         # Flatten the features if they are multi-dimensional (e.g., [batch, channels, height, width])\n",
    "#         layer1_features_flat = layer1_features.view(layer1_features.size(0), -1)\n",
    "#         features_4.append(layer1_features_flat.cpu())\n",
    "#         labels.append(targets.cpu())\n",
    "#         if len(labels) >500:\n",
    "#             break\n",
    "\n",
    "# # Concatenate all features and labels\n",
    "# features_4 = torch.cat(features_4, dim=0).numpy()\n",
    "# labels = torch.cat(labels, dim=0).numpy()\n",
    "\n",
    "# # Plotting\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "# # PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_features = pca.fit_transform(features_4)\n",
    "# print(\"PCA Dope \")\n",
    "# # PCA Plot\n",
    "# axes[0].scatter(pca_features[:, 0], pca_features[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# axes[0].set_title(\"PCA Feature Visualization\")\n",
    "# axes[0].set_xlabel(\"Dimension 1\")\n",
    "# axes[0].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# # t-SNE\n",
    "# tsne = TSNE(n_components=2, perplexity=30, random_state=0)\n",
    "# tsne_features = tsne.fit_transform(features_4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # t-SNE Plot\n",
    "# axes[1].scatter(tsne_features[:, 0], tsne_features[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# axes[1].set_title(\"t-SNE Feature Visualization\")\n",
    "# axes[1].set_xlabel(\"Dimension 1\")\n",
    "# axes[1].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# plt.colorbar(plt.cm.ScalarMappable(cmap='tab10'), ax=axes, location='right', label=\"Class\")\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"layer 4.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # Assume `model` is your ResNet instance and `data_loader` is your DataLoader with 100 samples from 10 classes.\n",
    "\n",
    "# # Initialize storage for features and labels\n",
    "# features_3 = []\n",
    "# labels = []\n",
    "# layer_index = 3\n",
    "# # Extract features from layer 1\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs = inputs.cuda()\n",
    "#         layer1_features = model.intermediate_forward(inputs, layer_index=layer_index)\n",
    "#         # Flatten the features if they are multi-dimensional (e.g., [batch, channels, height, width])\n",
    "#         layer1_features_flat = layer1_features.view(layer1_features.size(0), -1)\n",
    "#         features_3.append(layer1_features_flat.cpu())\n",
    "#         labels.append(targets.cpu())\n",
    "#         if len(labels) >500:\n",
    "#             break\n",
    "\n",
    "# # Concatenate all features and labels\n",
    "# features_3 = torch.cat(features_3, dim=0).numpy()\n",
    "# labels = torch.cat(labels, dim=0).numpy()\n",
    "\n",
    "# # Plotting\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "# # PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_features = pca.fit_transform(features_3)\n",
    "# print(\"PCA Dope \")\n",
    "# # PCA Plot\n",
    "# axes[0].scatter(pca_features[:, 0], pca_features[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# axes[0].set_title(\"PCA Feature Visualization layer 3\")\n",
    "# axes[0].set_xlabel(\"Dimension 1\")\n",
    "# axes[0].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# # t-SNE\n",
    "# tsne = TSNE(n_components=2, perplexity=30, random_state=0)\n",
    "# tsne_features = tsne.fit_transform(features_3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # t-SNE Plot\n",
    "# axes[1].scatter(tsne_features[:, 0], tsne_features[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# axes[1].set_title(\"t-SNE Feature Visualization layer 3\")\n",
    "# axes[1].set_xlabel(\"Dimension 1\")\n",
    "# axes[1].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# plt.colorbar(plt.cm.ScalarMappable(cmap='tab10'), ax=axes, location='right', label=\"Class\")\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"layer 3.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # Assume `model` is your ResNet instance and `data_loader` is your DataLoader with 100 samples from 10 classes.\n",
    "\n",
    "# # Initialize storage for features and labels\n",
    "# features_2 = []\n",
    "# labels = []\n",
    "# layer_index = 2\n",
    "# # Extract features from layer 1\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs = inputs.cuda()\n",
    "#         layer1_features = model.intermediate_forward(inputs, layer_index=layer_index)\n",
    "#         # Flatten the features if they are multi-dimensional (e.g., [batch, channels, height, width])\n",
    "#         layer1_features_flat = layer1_features.view(layer1_features.size(0), -1)\n",
    "#         features_2.append(layer1_features_flat.cpu())\n",
    "#         labels.append(targets.cpu())\n",
    "#         if len(labels) >500:\n",
    "#             break\n",
    "\n",
    "# # Concatenate all features and labels\n",
    "# features_2 = torch.cat(features_2, dim=0).numpy()\n",
    "# labels = torch.cat(labels, dim=0).numpy()\n",
    "\n",
    "# # Plotting\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "# # PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_features = pca.fit_transform(features_2)\n",
    "# print(\"PCA Dope \")\n",
    "# # PCA Plot\n",
    "# axes[0].scatter(pca_features[:, 0], pca_features[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# axes[0].set_title(\"PCA Feature Visualization layer 2\")\n",
    "# axes[0].set_xlabel(\"Dimension 1\")\n",
    "# axes[0].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# # t-SNE\n",
    "# tsne = TSNE(n_components=2, perplexity=30, random_state=0)\n",
    "# tsne_features = tsne.fit_transform(features_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # t-SNE Plot\n",
    "# axes[1].scatter(tsne_features[:, 0], tsne_features[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# axes[1].set_title(\"t-SNE Feature Visualization layer 2\")\n",
    "# axes[1].set_xlabel(\"Dimension 1\")\n",
    "# axes[1].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# plt.colorbar(plt.cm.ScalarMappable(cmap='tab10'), ax=axes, location='right', label=\"Class\")\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"layer 2.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# # Assume `model` is your ResNet instance and `data_loader` is your DataLoader with 100 samples from 10 classes.\n",
    "\n",
    "# # Initialize storage for features and labels\n",
    "# features_1 = []\n",
    "# labels = []\n",
    "# layer_index = 1\n",
    "# # Extract features from layer 1\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for inputs, targets in train_loader:\n",
    "#         inputs = inputs.cuda()\n",
    "#         layer1_features = model.intermediate_forward(inputs, layer_index=layer_index)\n",
    "#         # Flatten the features if they are multi-dimensional (e.g., [batch, channels, height, width])\n",
    "#         layer1_features_flat = layer1_features.view(layer1_features.size(0), -1)\n",
    "#         features_1.append(layer1_features_flat.cpu())\n",
    "#         labels.append(targets.cpu())\n",
    "#         if len(labels) >500:\n",
    "#             break\n",
    "\n",
    "# # Concatenate all features and labels\n",
    "# features_1 = torch.cat(features_1, dim=0).numpy()\n",
    "# labels = torch.cat(labels, dim=0).numpy()\n",
    "\n",
    "# # Plotting\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "# # PCA\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_features = pca.fit_transform(features_1)\n",
    "# print(\"PCA Dope \")\n",
    "# # PCA Plot\n",
    "# axes[0].scatter(pca_features[:, 0], pca_features[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# axes[0].set_title(\"PCA Feature Visualization layer 1\")\n",
    "# axes[0].set_xlabel(\"Dimension 1\")\n",
    "# axes[0].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# # t-SNE\n",
    "# tsne = TSNE(n_components=2, perplexity=30, random_state=0)\n",
    "# tsne_features = tsne.fit_transform(features_1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # t-SNE Plot\n",
    "# axes[1].scatter(tsne_features[:, 0], tsne_features[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "# axes[1].set_title(\"t-SNE Feature Visualization layer 1\")\n",
    "# axes[1].set_xlabel(\"Dimension 1\")\n",
    "# axes[1].set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# plt.colorbar(plt.cm.ScalarMappable(cmap='tab10'), ax=axes, location='right', label=\"Class\")\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"layer 1.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-24.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
